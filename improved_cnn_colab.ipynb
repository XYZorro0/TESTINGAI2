{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPROVED CNN - Optimized for 70-85% Validation Accuracy\n",
    "\n",
    "## Key Improvements from Original Version\n",
    "\n",
    "This notebook implements an optimized CNN with critical improvements to boost validation accuracy from 44.85% to **70-85%**:\n",
    "\n",
    "### ðŸŽ¯ Main Changes\n",
    "\n",
    "1. **Reduced Dropout** (50% reduction)\n",
    "   - Conv blocks: 0.1, 0.15, 0.2, 0.25 (was 0.2, 0.3, 0.4, 0.4)\n",
    "   - FC layers: 0.3 (was 0.5)\n",
    "\n",
    "2. **Less Aggressive Augmentation**\n",
    "   - Rotation: 10Â° (was 15Â°)\n",
    "   - Color jitter: 0.1 (was 0.2)\n",
    "   - Affine translate: 0.08 (was 0.1)\n",
    "\n",
    "3. **Better Training Dynamics**\n",
    "   - NO Label Smoothing (standard CrossEntropy)\n",
    "   - OneCycleLR scheduler with 30% warmup\n",
    "   - Gradient clipping (max_norm=1.0)\n",
    "   - Higher learning rate: 0.003 (was 0.001)\n",
    "   - Lower weight decay: 5e-5 (was 1e-4)\n",
    "\n",
    "4. **Extended Training**\n",
    "   - 150 epochs (was 100)\n",
    "   - Patience: 20 (was 15)\n",
    "\n",
    "### ðŸ“Š Expected Results\n",
    "\n",
    "- **Training Accuracy**: 75-90%\n",
    "- **Validation Accuracy**: 70-85%\n",
    "- **Overfitting Gap**: 3-7%\n",
    "- **Training Time**: 3-5 hours on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install and Import\n",
    "!pip install torch torchvision tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Load Data from Pickle Files\n",
    "# Load data\n",
    "with open('train-70_.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('validation-10_.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_images = np.array(train_data['images'])\n",
    "train_labels = np.array(train_data['labels'])\n",
    "val_images = np.array(val_data['images'])\n",
    "val_labels = np.array(val_data['labels'])\n",
    "\n",
    "print(f\"Train: {train_images.shape}, Val: {val_images.shape}\")\n",
    "\n",
    "# Remap labels to [0, num_classes-1]\n",
    "unique_labels = np.unique(np.concatenate([train_labels, val_labels]))\n",
    "label_mapping = {old: new for new, old in enumerate(unique_labels)}\n",
    "train_labels = np.array([label_mapping[l] for l in train_labels])\n",
    "val_labels = np.array([label_mapping[l] for l in val_labels])\n",
    "\n",
    "print(f\"Remapped labels to range: [{train_labels.min()}, {train_labels.max()}]\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Custom Dataset Class\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading images from pickle files\"\"\"\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.classes = np.unique(labels).tolist()\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        if image.dtype == np.uint8:\n",
    "            image = Image.fromarray(image)\n",
    "        else:\n",
    "            image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: IMPROVED Model Architecture with Less Dropout\n",
    "class ImprovedTinyImageNetCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    IMPROVED CNN with optimized dropout and architecture\n",
    "    Key changes:\n",
    "    - Reduced dropout for better learning\n",
    "    - Wider filters in later layers\n",
    "    - Better regularization balance\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=15, dropout_rate=0.3):\n",
    "        super(ImprovedTinyImageNetCNN, self).__init__()\n",
    "\n",
    "        # Block 1: 64x64 -> 32x32 (REDUCED dropout: 0.1)\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.1)  # REDUCED from 0.2\n",
    "        )\n",
    "\n",
    "        # Block 2: 32x32 -> 16x16 (REDUCED dropout: 0.15)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.15)  # REDUCED from 0.3\n",
    "        )\n",
    "\n",
    "        # Block 3: 16x16 -> 8x8 (REDUCED dropout: 0.2)\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.2)  # REDUCED from 0.4\n",
    "        )\n",
    "\n",
    "        # Block 4: 8x8 -> 4x4 (REDUCED dropout: 0.25)\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.25)  # REDUCED from 0.4\n",
    "        )\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # IMPROVED classifier with less dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),  # 0.3 instead of 0.5\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),  # 0.3 instead of 0.5\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Weight initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization for ReLU\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Test model creation\n",
    "model = ImprovedTinyImageNetCNN(num_classes=15).to(device)\n",
    "print(f'\\nImproved Model created successfully!')\n",
    "print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: IMPROVED Data Transforms (Less Aggressive Augmentation)\n",
    "# IMPROVED: Less aggressive augmentation for better learning\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),  # REDUCED from 15\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),  # REDUCED\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.08, 0.08)),  # REDUCED from 0.1\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transform (no augmentation, only normalization)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomImageDataset(\n",
    "    images=train_images,\n",
    "    labels=train_labels,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = CustomImageDataset(\n",
    "    images=val_images,\n",
    "    labels=val_labels,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f'Number of classes: {train_dataset.num_classes}')\n",
    "print(f'Classes: {train_dataset.classes}')\n",
    "\n",
    "# IMPROVED Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 150  # INCREASED for better convergence\n",
    "LEARNING_RATE = 0.003  # INCREASED for faster learning\n",
    "PATIENCE = 20  # INCREASED patience\n",
    "WEIGHT_DECAY = 5e-5  # REDUCED weight decay\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'\\nTraining batches: {len(train_loader)}')\n",
    "print(f'Validation batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: IMPROVED Training Setup\n",
    "# Initialize model\n",
    "model = ImprovedTinyImageNetCNN(num_classes=15, dropout_rate=0.3).to(device)\n",
    "\n",
    "# IMPROVED: Use regular CrossEntropy (NO label smoothing)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# IMPROVED: Adam with better weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# IMPROVED: Use OneCycleLR for better training dynamics\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,  # 30% warmup\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=10000.0\n",
    ")\n",
    "\n",
    "print('IMPROVED Training setup complete!')\n",
    "print(f'Learning Rate: {LEARNING_RATE}')\n",
    "print(f'Weight Decay: {WEIGHT_DECAY}')\n",
    "print(f'Batch Size: {BATCH_SIZE}')\n",
    "print(f'Max Epochs: {NUM_EPOCHS}')\n",
    "print(f'Scheduler: OneCycleLR with cosine annealing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: IMPROVED Training Functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch with gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Step per batch for OneCycleLR\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: IMPROVED Training Loop\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "learning_rates = []\n",
    "\n",
    "print('Starting IMPROVED training...\\n')\n",
    "print('Key improvements:')\n",
    "print('âœ“ Reduced dropout (0.1-0.3 instead of 0.2-0.5)')\n",
    "print('âœ“ Less aggressive augmentation')\n",
    "print('âœ“ No label smoothing')\n",
    "print('âœ“ OneCycleLR scheduler with warmup')\n",
    "print('âœ“ Gradient clipping')\n",
    "print('âœ“ Higher initial learning rate')\n",
    "print('âœ“ Lower weight decay')\n",
    "print()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 70)\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    learning_rates.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "    print(f'Learning Rate: {learning_rates[-1]:.6f}')\n",
    "    print(f'Overfitting gap: {abs(train_acc - val_acc):.2f}%')\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "        }, 'model.pth')\n",
    "\n",
    "        print(f'âœ“ Best model saved! (Val Acc: {val_acc:.2f}%)')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'No improvement. Patience: {patience_counter}/{PATIENCE}')\n",
    "\n",
    "    print(f'Best Val Acc so far: {best_val_acc:.2f}% (Epoch {best_epoch})')\n",
    "    print('=' * 70 + '\\n')\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs!')\n",
    "        print(f'Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}')\n",
    "        break\n",
    "\n",
    "print('\\nTraining completed!')\n",
    "print(f'Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Plot Training History\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "ax1.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Train Acc', linewidth=2)\n",
    "ax2.plot(val_accs, label='Val Acc', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "ax3.plot(learning_rates, linewidth=2, color='green')\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax3.set_title('Learning Rate Schedule (OneCycleLR)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# Overfitting gap plot\n",
    "gaps = [abs(t - v) for t, v in zip(train_accs, val_accs)]\n",
    "ax4.plot(gaps, linewidth=2, color='red')\n",
    "ax4.set_xlabel('Epoch', fontsize=12)\n",
    "ax4.set_ylabel('Overfitting Gap (%)', fontsize=12)\n",
    "ax4.set_title('Train-Val Accuracy Gap', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axhline(y=5, color='orange', linestyle='--', label='Target (<5%)')\n",
    "ax4.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('improved_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal Training Accuracy: {train_accs[-1]:.2f}%')\n",
    "print(f'Final Validation Accuracy: {val_accs[-1]:.2f}%')\n",
    "print(f'Best Validation Accuracy: {best_val_acc:.2f}%')\n",
    "print(f'Final Overfitting Gap: {abs(train_accs[-1] - val_accs[-1]):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Submission Functions (Required for Instructor Testing)\n",
    "# For compatibility, create an alias to the original class name\n",
    "TinyImageNetCNN = ImprovedTinyImageNetCNN\n",
    "\n",
    "def load_model(model_path='model.pth', device='cuda'):\n",
    "    \"\"\"Load the trained model from file\"\"\"\n",
    "    model = ImprovedTinyImageNetCNN(num_classes=15, dropout_rate=0.3)\n",
    "\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f'Model loaded successfully from {model_path}')\n",
    "    print(f'Best validation accuracy during training: {checkpoint[\"val_acc\"]:.2f}%')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, test_loader, device='cuda'):\n",
    "    \"\"\"Make predictions on test data\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(test_loader, desc='Predicting'):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(all_predictions)\n",
    "\n",
    "\n",
    "# Test the load function\n",
    "print('\\nTesting load_model function...')\n",
    "loaded_model = load_model('model.pth', device=device)\n",
    "print('âœ“ Load function works correctly!')\n",
    "\n",
    "# Verify on validation set\n",
    "print('\\nTesting predict function on validation set...')\n",
    "predictions = predict(loaded_model, val_loader, device=device)\n",
    "true_labels = val_labels\n",
    "accuracy = 100. * np.sum(predictions == true_labels) / len(true_labels)\n",
    "\n",
    "print(f'\\nâœ“ Predict function works correctly!')\n",
    "print(f'Validation Accuracy with loaded model: {accuracy:.2f}%')\n",
    "print(f'Number of correct predictions: {np.sum(predictions == true_labels)}/{len(true_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Final Summary\n",
    "print('\\n' + '='*80)\n",
    "print('IMPROVED MODEL SUMMARY')\n",
    "print('='*80)\n",
    "print(f'Architecture: Improved Sequential CNN with Optimized Hyperparameters')\n",
    "print(f'Total Parameters: {sum(p.numel() for p in loaded_model.parameters()):,}')\n",
    "print(f'\\nKey Improvements:')\n",
    "print('  âœ“ Reduced Dropout: 0.1â†’0.15â†’0.2â†’0.25 (conv) + 0.3 (FC)')\n",
    "print('  âœ“ Less Aggressive Augmentation (rotation: 10Â°, jitter: 0.1)')\n",
    "print('  âœ“ NO Label Smoothing (standard CrossEntropy)')\n",
    "print('  âœ“ OneCycleLR Scheduler with 30% warmup')\n",
    "print('  âœ“ Gradient Clipping (max_norm=1.0)')\n",
    "print('  âœ“ Higher Learning Rate (0.003)')\n",
    "print('  âœ“ Lower Weight Decay (5e-5)')\n",
    "print('  âœ“ Increased Patience (20 epochs)')\n",
    "print(f'\\nPerformance:')\n",
    "print(f'  Best Validation Accuracy: {best_val_acc:.2f}%')\n",
    "print(f'  Achieved at Epoch: {best_epoch}')\n",
    "print(f'  Expected Improvement: +20-35% from original 44.85%')\n",
    "print('='*80)\n",
    "\n",
    "print('\\nâœ… READY FOR SUBMISSION!')\n",
    "print('\\nDownload these files from Colab:')\n",
    "print('1. This notebook (.ipynb)')\n",
    "print('2. model.pth')\n",
    "print('\\nZip them as: GroupX_Assignment.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
